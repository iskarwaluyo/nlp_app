word2 = sapply(strsplit(names(sums_n2), "_", fixed = TRUE), '[[', 2),
count = sums_n2)
n3gram_counts <<- data.table(
word1 = sapply(strsplit(names(sums_n3), "_", fixed = TRUE), '[[', 1),
word2 = sapply(strsplit(names(sums_n3), "_", fixed = TRUE), '[[', 2),
word3 = sapply(strsplit(names(sums_n3), "_", fixed = TRUE), '[[', 3),
count = sums_n3)
setkey(n1gram_counts, word1)
setkey(n2gram_counts, word1, word2)
setkey(n3gram_counts, word1, word2, word3)
}
ngram_create <- function(use_clean){
if (use_clean == 1){corpus_tokens <<- sample_corpus_tokens_clean}
else{corpus_tokens <<- sample_corpus_tokens}
# PART III: NATURAL LANGUAGE PROCESSING WITH "quanteda" and "tm" PACKAGE
dfm_corpus_tokens <<- dfm(corpus_tokens){
} # DFM QUANTEDA PACKAGE
# TERM FREQUENCE (TF) CALCULATES THE OCCURENCE OF TERMS IN A DOCUMENT
# TF(t) = c(t,d)
# t = text, d = document
# N-GRAM TOKENIZATION (TOKENIZING FULL PHRASES)
ngram_1 <- tokens_ngrams(corpus_tokens, n = 1)
ngram_2 <- tokens_ngrams(corpus_tokens, n = 2)
ngram_3 <- tokens_ngrams(corpus_tokens, n = 3)
dfm_n1 <- dfm(ngram_1)
dfm_n2 <- dfm(ngram_2)
dfm_n3 <- dfm(ngram_3)
# dfm_n1_trim <- dfm_trim(dfm_n1, 10)
# dfm_n2_trim <- dfm_trim(dfm_n2, 10)
# dfm_n3_trim <- dfm_trim(dfm_n3, 10)
# Create named vectors with counts of words
sums_n1 <- colSums(dfm_n1)
sums_n2 <- colSums(dfm_n2)
sums_n3 <- colSums(dfm_n3)
# Create data tables with individual words as columns
n1gram_counts <<- data.table(word1 = names(sums_n1), count = sums_n1)
n2gram_counts <<- data.table(
word1 = sapply(strsplit(names(sums_n2), "_", fixed = TRUE), '[[', 1),
word2 = sapply(strsplit(names(sums_n2), "_", fixed = TRUE), '[[', 2),
count = sums_n2)
n3gram_counts <<- data.table(
word1 = sapply(strsplit(names(sums_n3), "_", fixed = TRUE), '[[', 1),
word2 = sapply(strsplit(names(sums_n3), "_", fixed = TRUE), '[[', 2),
word3 = sapply(strsplit(names(sums_n3), "_", fixed = TRUE), '[[', 3),
count = sums_n3)
setkey(n1gram_counts, word1)
setkey(n2gram_counts, word1, word2)
setkey(n3gram_counts, word1, word2, word3)
}
ngram_create <- function(use_clean){
if (use_clean == 1){corpus_tokens <<- sample_corpus_tokens_clean}
else{corpus_tokens <<- sample_corpus_tokens}
# PART III: NATURAL LANGUAGE PROCESSING WITH "quanteda" and "tm" PACKAGE
dfm_corpus_tokens <<- dfm(corpus_tokens)
# TERM FREQUENCE (TF) CALCULATES THE OCCURENCE OF TERMS IN A DOCUMENT
# TF(t) = c(t,d)
# t = text, d = document
# N-GRAM TOKENIZATION (TOKENIZING FULL PHRASES)
ngram_1 <- tokens_ngrams(corpus_tokens, n = 1)
ngram_2 <- tokens_ngrams(corpus_tokens, n = 2)
ngram_3 <- tokens_ngrams(corpus_tokens, n = 3)
dfm_n1 <- dfm(ngram_1)
dfm_n2 <- dfm(ngram_2)
dfm_n3 <- dfm(ngram_3)
# dfm_n1_trim <- dfm_trim(dfm_n1, 10)
# dfm_n2_trim <- dfm_trim(dfm_n2, 10)
# dfm_n3_trim <- dfm_trim(dfm_n3, 10)
# Create named vectors with counts of words
sums_n1 <- colSums(dfm_n1)
sums_n2 <- colSums(dfm_n2)
sums_n3 <- colSums(dfm_n3)
# Create data tables with individual words as columns
n1gram_counts <<- data.table(word1 = names(sums_n1), count = sums_n1)
n2gram_counts <<- data.table(
word1 = sapply(strsplit(names(sums_n2), "_", fixed = TRUE), '[[', 1),
word2 = sapply(strsplit(names(sums_n2), "_", fixed = TRUE), '[[', 2),
count = sums_n2)
n3gram_counts <<- data.table(
word1 = sapply(strsplit(names(sums_n3), "_", fixed = TRUE), '[[', 1),
word2 = sapply(strsplit(names(sums_n3), "_", fixed = TRUE), '[[', 2),
word3 = sapply(strsplit(names(sums_n3), "_", fixed = TRUE), '[[', 3),
count = sums_n3)
setkey(n1gram_counts, word1)
setkey(n2gram_counts, word1, word2)
setkey(n3gram_counts, word1, word2, word3)
}
ngram_create(1)
ngram_plots <- function(n_gram){
if(n_gram == 1){
graph.data <- n1gram_counts[order(n1gram_counts$count, decreasing = T), ]
graph.data <- graph.data[1:20, ]
graph.data$word <- factor(graph.data$word1, levels = graph.data$word1)
ggplot(data=graph.data, aes(x=word, y=count), position = fill(reverse = TRUE)) +
geom_bar(stat="identity") +
coord_flip() +
labs(title="Top 20 unigrams used in Sample Corpus",
subtitle="Data from 2019",
y="Number of unigrams",
x="Unigrams",
caption = "Data source: Courseara Data Science Specialization") +
theme(legend.position = "bottom")
}
else if(n_gram == 2) {
graph.data <- n2gram_counts[order(n2gram_counts$count, decreasing = T), ]
graph.data <- graph.data[1:20, ]
graph.data$word <- paste(graph.data$word1, graph.data$word2)
graph.data$word <- factor(graph.data$word, levels = graph.data$word)
ggplot(data=graph.data, aes(x=word, y=count), position = fill(reverse = TRUE)) +
geom_bar(stat="identity") +
coord_flip() +
labs(title="Top 20 bigrams used in Sample Corpus",
subtitle="Data from 2019",
y="Number of bigrams",
x="Bigrams",
caption = "Data source: Courseara Data Science Specialization") +
theme(legend.position = "bottom")
}
else if(n_gram == 3) {
graph.data <- n3gram_counts[order(n3gram_counts$count, decreasing = T), ]
graph.data <- graph.data[1:20, ]
graph.data$word <- paste(graph.data$word1, graph.data$word2, graph.data$word3)
graph.data$word <- factor(graph.data$word, levels = graph.data$word)
ggplot(data=graph.data, aes(x=word, y=count), position = fill(reverse = TRUE)) +
geom_bar(stat="identity") +
coord_flip() +
labs(title="Top 20 trigrams used in Sample Corpus",
subtitle="Data from 2019",
y="Number of trigrams",
x="Trigrams",
caption = "Data source: Courseara Data Science Specialization") +
theme(legend.position = "bottom")
}
else {print("The number of grams should be 1 to 3 only.")}
}
ngram_plots(2)
ngram_plots <- function(n_gram){
if(n_gram == 1){
graph.data <- n1gram_counts[order(n1gram_counts$count, decreasing = T), ]
graph.data <- graph.data[1:20, ]
graph.data$word <- factor(graph.data$word1, levels = graph.data$word1)
ggplot(data=graph.data, aes(x=word, y=count), position = fill(reverse = TRUE)) +
geom_bar(stat="identity") +
coord_flip() +
labs(title= paste0("Top 20 unigrams used in", corpus_n),
subtitle="Data from 2019",
y="Number of unigrams",
x="Unigrams",
caption = "Data source: Courseara Data Science Specialization") +
theme(legend.position = "bottom")
}
else if(n_gram == 2) {
graph.data <- n2gram_counts[order(n2gram_counts$count, decreasing = T), ]
graph.data <- graph.data[1:20, ]
graph.data$word <- paste(graph.data$word1, graph.data$word2)
graph.data$word <- factor(graph.data$word, levels = graph.data$word)
ggplot(data=graph.data, aes(x=word, y=count), position = fill(reverse = TRUE)) +
geom_bar(stat="identity") +
coord_flip() +
labs(title= paste0("Top 20 unigrams used in", corpus_n),
subtitle="Data from 2019",
y="Number of bigrams",
x="Bigrams",
caption = "Data source: Courseara Data Science Specialization") +
theme(legend.position = "bottom")
}
else if(n_gram == 3) {
graph.data <- n3gram_counts[order(n3gram_counts$count, decreasing = T), ]
graph.data <- graph.data[1:20, ]
graph.data$word <- paste(graph.data$word1, graph.data$word2, graph.data$word3)
graph.data$word <- factor(graph.data$word, levels = graph.data$word)
ggplot(data=graph.data, aes(x=word, y=count), position = fill(reverse = TRUE)) +
geom_bar(stat="identity") +
coord_flip() +
labs(title= paste0("Top 20 unigrams used in", corpus_n),
subtitle="Data from 2019",
y="Number of trigrams",
x="Trigrams",
caption = "Data source: Courseara Data Science Specialization") +
theme(legend.position = "bottom")
}
else {print("The number of grams should be 1 to 3 only.")}
}
ngram_plots(1)
corpus_tokeinze <- function(corpus_n, remove_stepwords, word_stemming){
corpus_n <<- corpus_n
corpus_select <- split_corpus[[corpus_n]]
sample_corpus_tokens <<- tokens(corpus_select)
sample_corpus_tokens_clean <<- tokens(
x = tolower(corpus_select), remove_punct = TRUE, remove_twitter = TRUE,
remove_numbers = TRUE, remove_hyphens = TRUE, remove_symbols = TRUE,
remove_url = TRUE
)
if (remove_stepwords == 1){
sample_corpus_tokens_clean <<- tokens_wordstem(sample_corpus_tokens_clean, language = "english")
}
else{sample_corpus_tokens_clean <- sample_corpus_tokens_clean}
if (word_stemming == 1){
sample_corpus_tokens_clean <<- tokens_remove(sample_corpus_tokens_clean, c(stopwords("english")))
}
else{sample_corpus_tokens_clean <- sample_corpus_tokens_clean}
}
ngram_create <- function(use_clean){
if (use_clean == 1){corpus_tokens <<- sample_corpus_tokens_clean}
else{corpus_tokens <<- sample_corpus_tokens}
# PART III: NATURAL LANGUAGE PROCESSING WITH "quanteda" and "tm" PACKAGE
dfm_corpus_tokens <<- dfm(corpus_tokens)
# TERM FREQUENCE (TF) CALCULATES THE OCCURENCE OF TERMS IN A DOCUMENT
# TF(t) = c(t,d)
# t = text, d = document
# N-GRAM TOKENIZATION (TOKENIZING FULL PHRASES)
ngram_1 <- tokens_ngrams(corpus_tokens, n = 1)
ngram_2 <- tokens_ngrams(corpus_tokens, n = 2)
ngram_3 <- tokens_ngrams(corpus_tokens, n = 3)
dfm_n1 <- dfm(ngram_1)
dfm_n2 <- dfm(ngram_2)
dfm_n3 <- dfm(ngram_3)
# dfm_n1_trim <- dfm_trim(dfm_n1, 10)
# dfm_n2_trim <- dfm_trim(dfm_n2, 10)
# dfm_n3_trim <- dfm_trim(dfm_n3, 10)
# Create named vectors with counts of words
sums_n1 <- colSums(dfm_n1)
sums_n2 <- colSums(dfm_n2)
sums_n3 <- colSums(dfm_n3)
# Create data tables with individual words as columns
n1gram_counts <<- data.table(word1 = names(sums_n1), count = sums_n1)
n2gram_counts <<- data.table(
word1 = sapply(strsplit(names(sums_n2), "_", fixed = TRUE), '[[', 1),
word2 = sapply(strsplit(names(sums_n2), "_", fixed = TRUE), '[[', 2),
count = sums_n2)
n3gram_counts <<- data.table(
word1 = sapply(strsplit(names(sums_n3), "_", fixed = TRUE), '[[', 1),
word2 = sapply(strsplit(names(sums_n3), "_", fixed = TRUE), '[[', 2),
word3 = sapply(strsplit(names(sums_n3), "_", fixed = TRUE), '[[', 3),
count = sums_n3)
setkey(n1gram_counts, word1)
setkey(n2gram_counts, word1, word2)
setkey(n3gram_counts, word1, word2, word3)
}
corpus_tokeinze(5, 1, 1)
ngram_plots(1)
ngram_plots <- function(n_gram){
if(n_gram == 1){
graph.data <- n1gram_counts[order(n1gram_counts$count, decreasing = T), ]
graph.data <- graph.data[1:20, ]
graph.data$word <- factor(graph.data$word1, levels = graph.data$word1)
ggplot(data=graph.data, aes(x=word, y=count), position = fill(reverse = TRUE)) +
geom_bar(stat="identity") +
coord_flip() +
labs(title= paste0("Top 20 unigrams used in sample corpus chunk", corpus_n),
subtitle="Data from 2019",
y="Number of unigrams",
x="Unigrams",
caption = paste0("Data source: ", data_source, language_select, "from Coursera Data Science Specialization")) +
theme(legend.position = "bottom")
}
else if(n_gram == 2) {
graph.data <- n2gram_counts[order(n2gram_counts$count, decreasing = T), ]
graph.data <- graph.data[1:20, ]
graph.data$word <- paste(graph.data$word1, graph.data$word2)
graph.data$word <- factor(graph.data$word, levels = graph.data$word)
ggplot(data=graph.data, aes(x=word, y=count), position = fill(reverse = TRUE)) +
geom_bar(stat="identity") +
coord_flip() +
labs(title= paste0("Top 20 unigrams used in", corpus_n),
subtitle="Data from 2019",
y="Number of bigrams",
x="Bigrams",
caption = paste0("Data source: ", data_source, language_select, "from Coursera Data Science Specialization")) +      theme(legend.position = "bottom")
}
else if(n_gram == 3) {
graph.data <- n3gram_counts[order(n3gram_counts$count, decreasing = T), ]
graph.data <- graph.data[1:20, ]
graph.data$word <- paste(graph.data$word1, graph.data$word2, graph.data$word3)
graph.data$word <- factor(graph.data$word, levels = graph.data$word)
ggplot(data=graph.data, aes(x=word, y=count), position = fill(reverse = TRUE)) +
geom_bar(stat="identity") +
coord_flip() +
labs(title= paste0("Top 20 unigrams used in", corpus_n),
subtitle="Data from 2019",
y="Number of trigrams",
x="Trigrams",
caption = paste0("Data source: ", data_source, language_select, "from Coursera Data Science Specialization")) +      theme(legend.position = "bottom")
}
else {print("The number of grams should be 1 to 3 only.")}
}
ngram_plots(1)
ngram_plots <- function(n_gram){
if(n_gram == 1){
graph.data <- n1gram_counts[order(n1gram_counts$count, decreasing = T), ]
graph.data <- graph.data[1:20, ]
graph.data$word <- factor(graph.data$word1, levels = graph.data$word1)
ggplot(data=graph.data, aes(x=word, y=count), position = fill(reverse = TRUE)) +
geom_bar(stat="identity") +
coord_flip() +
labs(title= paste0("Top 20 unigrams used in sample corpus chunk", corpus_n),
subtitle="Data from 2019",
y="Number of unigrams",
x="Unigrams",
caption = paste0("Data source: ", data_source, "in ", language_select, " from Coursera Data Science Specialization")) +
theme(legend.position = "bottom")
}
else if(n_gram == 2) {
graph.data <- n2gram_counts[order(n2gram_counts$count, decreasing = T), ]
graph.data <- graph.data[1:20, ]
graph.data$word <- paste(graph.data$word1, graph.data$word2)
graph.data$word <- factor(graph.data$word, levels = graph.data$word)
ggplot(data=graph.data, aes(x=word, y=count), position = fill(reverse = TRUE)) +
geom_bar(stat="identity") +
coord_flip() +
labs(title= paste0("Top 20 unigrams used in", corpus_n),
subtitle="Data from 2019",
y="Number of bigrams",
x="Bigrams",
caption = paste0("Data source: ", data_source, "in ", language_select, " from Coursera Data Science Specialization")) +
theme(legend.position = "bottom")
}
else if(n_gram == 3) {
graph.data <- n3gram_counts[order(n3gram_counts$count, decreasing = T), ]
graph.data <- graph.data[1:20, ]
graph.data$word <- paste(graph.data$word1, graph.data$word2, graph.data$word3)
graph.data$word <- factor(graph.data$word, levels = graph.data$word)
ggplot(data=graph.data, aes(x=word, y=count), position = fill(reverse = TRUE)) +
geom_bar(stat="identity") +
coord_flip() +
labs(title= paste0("Top 20 unigrams used in", corpus_n),
subtitle="Data from 2019",
y="Number of trigrams",
x="Trigrams",
caption = paste0("Data source: ", data_source, "in ", language_select, " from Coursera Data Science Specialization")) +
theme(legend.position = "bottom")
}
else {print("The number of grams should be 1 to 3 only.")}
}
ngram_plots(1)
ngram_plots <- function(n_gram){
if(n_gram == 1){
graph.data <- n1gram_counts[order(n1gram_counts$count, decreasing = T), ]
graph.data <- graph.data[1:20, ]
graph.data$word <- factor(graph.data$word1, levels = graph.data$word1)
ggplot(data=graph.data, aes(x=word, y=count), position = fill(reverse = TRUE)) +
geom_bar(stat="identity") +
coord_flip() +
labs(title= paste0("Top 20 unigrams used in sample corpus chunk", corpus_n),
subtitle="Data from 2019",
y="Number of unigrams",
x="Unigrams",
caption = paste0("Data source: ", data_source, " in ", language_select, " from Coursera Data Science Specialization")) +
theme(legend.position = "bottom")
}
else if(n_gram == 2) {
graph.data <- n2gram_counts[order(n2gram_counts$count, decreasing = T), ]
graph.data <- graph.data[1:20, ]
graph.data$word <- paste(graph.data$word1, graph.data$word2)
graph.data$word <- factor(graph.data$word, levels = graph.data$word)
ggplot(data=graph.data, aes(x=word, y=count), position = fill(reverse = TRUE)) +
geom_bar(stat="identity") +
coord_flip() +
labs(title= paste0("Top 20 unigrams used in", corpus_n),
subtitle="Data from 2019",
y="Number of bigrams",
x="Bigrams",
caption = paste0("Data source: ", data_source, " in ", language_select, " from Coursera Data Science Specialization")) +
theme(legend.position = "bottom")
}
else if(n_gram == 3) {
graph.data <- n3gram_counts[order(n3gram_counts$count, decreasing = T), ]
graph.data <- graph.data[1:20, ]
graph.data$word <- paste(graph.data$word1, graph.data$word2, graph.data$word3)
graph.data$word <- factor(graph.data$word, levels = graph.data$word)
ggplot(data=graph.data, aes(x=word, y=count), position = fill(reverse = TRUE)) +
geom_bar(stat="identity") +
coord_flip() +
labs(title= paste0("Top 20 unigrams used in", corpus_n),
subtitle="Data from 2019",
y="Number of trigrams",
x="Trigrams",
caption = paste0("Data source: ", data_source, " in ", language_select, " from Coursera Data Science Specialization")) +
theme(legend.position = "bottom")
}
else {print("The number of grams should be 1 to 3 only.")}
}
ngram_plots(1)
shiny::runApp('/media/iskar/archivos/BIBLIOTECA/Programacion/R/SUPERMAS/R/archivosR/shinyappsupload/ReporteTicketsA')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('/media/iskar/archivos/BIBLIOTECA/Programacion/R/SUPERMAS/R/archivosR/shinyappsupload/ReporteTicketsA')
runApp('/media/iskar/archivos/BIBLIOTECA/Programacion/R/SUPERMAS/R/archivosR/shinyappcoursera')
runApp('/media/iskar/archivos/BIBLIOTECA/Programacion/R/SUPERMAS/R/archivosR/shinyappsupload/ReporteTicketsA')
runApp('/media/iskar/archivos/BIBLIOTECA/Programacion/R/SUPERMAS/R/archivosR/shinyappsupload/ReporteTicketsA')
runApp('/media/iskar/archivos/BIBLIOTECA/Programacion/R/SUPERMAS/R/archivosR/shinyappsupload/ReporteTicketsA')
runApp('/media/iskar/archivos/R/ticket_app')
runApp('app2')
runApp('app2')
runApp('/media/iskar/archivos/R/ticket_app')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
head(split_data)
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp()
runApp('app2')
runApp('app2')
data_source <<- "twitter"
directory <<- "/media/iskar/archivos/R/DATA_SCIENCE_CAPSTONE/datos/final/"
data_source <<- "twitter"
language_select <<- "en_US"
class(data_source)
runApp('app2')
class(data_source)
runApp('app2')
filename <<- paste0(directory, language_select, "/", language_select, ".", data_source, ".txt")
filename <<- paste0(directory, language_select, "/", language_select, ".", data_source, ".txt")
filename
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
## Only run examples in interactive R sessions
if (interactive()) {
ui <- fluidPage(
textInput("caption", "Caption", "Data Summary"),
verbatimTextOutput("value")
)
server <- function(input, output) {
output$value <- renderText({ input$caption })
}
shinyApp(ui, server)
}
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp()
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp()
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
runApp('app2')
